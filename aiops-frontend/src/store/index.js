import Vue from 'vue'
import Vuex from 'vuex'
Vue.use(Vuex)
const state = {
    isCollapse: false,
    logData: [
        {
            data: [{time: "2020-12-27T02:01:46.002Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-jar file on the remote FS is hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609033008462_0023/job.jar"}, {time: "2020-12-27T02:01:46.004Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: The job-conf file on the remote FS is /tmp/hadoop-yarn/staging/root/.staging/job_1609033008462_0023/job.xml"}, {time: "2020-12-27T02:01:46.005Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Size of containertokens_dob is 1"}, {time: "2020-12-27T02:01:46.005Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Putting shuffle token in serviceData"}, {time: "2020-12-27T02:01:46.005Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Adding #0 tokens and #1 secret keys for NM use for launching container"}, {time: "2020-12-27T02:01:46.023Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED"}, {time: "2020-12-27T02:01:46.024Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave1 to /default-rack"}, {time: "2020-12-27T02:01:46.025Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_0 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED"}, {time: "2020-12-27T02:01:46.026Z", level: "INFO", log: "[ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1609033008462_0023_01_000002 taskAttempt attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:46.026Z", level: "INFO", log: "[ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1609033008462_0023_01_000003 taskAttempt attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:46.028Z", level: "INFO", log: "[ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:46.028Z", level: "INFO", log: "[ContainerLauncher #1] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : hadoop-slave1:39617"}, {time: "2020-12-27T02:01:46.028Z", level: "INFO", log: "[ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:46.042Z", level: "INFO", log: "[ContainerLauncher #0] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : hadoop-slave1:39617"}, {time: "2020-12-27T02:01:46.068Z", level: "INFO", log: "[ContainerLauncher #1] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1609033008462_0023_m_000001_0 : 13562"}, {time: "2020-12-27T02:01:46.068Z", level: "INFO", log: "[ContainerLauncher #0] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1609033008462_0023_m_000000_0 : 13562"}, {time: "2020-12-27T02:01:46.070Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1609033008462_0023_m_000000_0] using containerId: [container_1609033008462_0023_01_000002 on NM: [hadoop-slave1:39617]"}, {time: "2020-12-27T02:01:46.072Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_0 TaskAttempt Transitioned from ASSIGNED to RUNNING"}, {time: "2020-12-27T02:01:46.072Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1609033008462_0023_m_000000"}, {time: "2020-12-27T02:01:46.072Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_0 TaskAttempt Transitioned from ASSIGNED to RUNNING"}, {time: "2020-12-27T02:01:46.072Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1609033008462_0023_m_000000 Task Transitioned from SCHEDULED to RUNNING"}, {time: "2020-12-27T02:01:46.072Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1609033008462_0023_m_000001_0] using containerId: [container_1609033008462_0023_01_000003 on NM: [hadoop-slave1:39617]"}, {time: "2020-12-27T02:01:46.072Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1609033008462_0023_m_000001"}, {time: "2020-12-27T02:01:46.073Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: task_1609033008462_0023_m_000001 Task Transitioned from SCHEDULED to RUNNING"}, {time: "2020-12-27T02:01:46.968Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1609033008462_0023: ask=4 release= 0 newContainers=0 finishedContainers=0 resourcelimit=<memory:12288, vCores:1> knownNMs=2"}, {time: "2020-12-27T02:01:47.026Z", level: "INFO", log: "[main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties"}, {time: "2020-12-27T02:01:47.051Z", level: "INFO", log: "[main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties"}, {time: "2020-12-27T02:01:47.125Z", level: "INFO", log: "[main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s)."}, {time: "2020-12-27T02:01:47.125Z", level: "INFO", log: "[main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started"}, {time: "2020-12-27T02:01:47.129Z", level: "INFO", log: "[main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s)."}, {time: "2020-12-27T02:01:47.129Z", level: "INFO", log: "[main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started"}, {time: "2020-12-27T02:01:47.137Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1609033008462_0023, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@6f2297aa)"}, {time: "2020-12-27T02:01:47.137Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:"}, {time: "2020-12-27T02:01:47.140Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1609033008462_0023, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@6a83bdcf)"}, {time: "2020-12-27T02:01:47.140Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:"}, {time: "2020-12-27T02:01:47.208Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now."}, {time: "2020-12-27T02:01:47.211Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now."}, {time: "2020-12-27T02:01:47.294Z", level: "INFO", log: "[Socket Reader #1 for port 36781] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1609033008462_0023 (auth:SIMPLE)"}, {time: "2020-12-27T02:01:47.297Z", level: "INFO", log: "[Socket Reader #1 for port 36781] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1609033008462_0023 (auth:SIMPLE)"}, {time: "2020-12-27T02:01:47.309Z", level: "INFO", log: "[IPC Server handler 0 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1609033008462_0023_m_000002 asked for a task"}, {time: "2020-12-27T02:01:47.309Z", level: "INFO", log: "[IPC Server handler 1 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID : jvm_1609033008462_0023_m_000003 asked for a task"}, {time: "2020-12-27T02:01:47.309Z", level: "INFO", log: "[IPC Server handler 1 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1609033008462_0023_m_000003 given task: attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:47.309Z", level: "INFO", log: "[IPC Server handler 0 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: JVM with ID: jvm_1609033008462_0023_m_000002 given task: attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:47.390Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609033008462_0023"}, {time: "2020-12-27T02:01:47.393Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609033008462_0023"}, {time: "2020-12-27T02:01:47.704Z", level: "INFO", log: "[main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id"}, {time: "2020-12-27T02:01:47.753Z", level: "INFO", log: "[main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id"}, {time: "2020-12-27T02:01:48.029Z", level: "INFO", log: "[main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 1"}, {time: "2020-12-27T02:01:48.037Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]"}, {time: "2020-12-27T02:01:48.080Z", level: "INFO", log: "[main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: File Output Committer Algorithm version is 1"}, {time: "2020-12-27T02:01:48.088Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.Task:  Using ResourceCalculatorProcessTree : [ ]"}, {time: "2020-12-27T02:01:48.156Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.MapTask: Processing split: hdfs://hadoop-master:9000/benchmarks/MRBench/mr_input/input_-1312372914.txt:0+2"}, {time: "2020-12-27T02:01:48.178Z", level: "WARN", log: "[main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.181Z", level: "INFO", log: "[IPC Server handler 0 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1609033008462_0023_m_000000_0 is : 0.0"}, {time: "2020-12-27T02:01:48.184Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task"}, {time: "2020-12-27T02:01:48.188Z", level: "WARN", log: "[main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://hadoop-master:9000/benchmarks/MRBench/mr_output/output_-261253429/_temporary/1/_temporary/attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:48.190Z", level: "FATAL", log: "[IPC Server handler 1 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1609033008462_0023_m_000000_0 - exited : java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.190Z", level: "INFO", log: "[IPC Server handler 1 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1609033008462_0023_m_000000_0: Error: java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.191Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1609033008462_0023_m_000000_0: Error: java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_-1312372914.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.192Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP"}, {time: "2020-12-27T02:01:48.194Z", level: "INFO", log: "[ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1609033008462_0023_01_000002 taskAttempt attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:48.194Z", level: "INFO", log: "[ContainerLauncher #2] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:48.194Z", level: "INFO", log: "[ContainerLauncher #2] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : hadoop-slave1:39617"}, {time: "2020-12-27T02:01:48.206Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP"}, {time: "2020-12-27T02:01:48.207Z", level: "INFO", log: "[CommitterEvent Processor #1] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT"}, {time: "2020-12-27T02:01:48.215Z", level: "WARN", log: "[CommitterEvent Processor #1] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://hadoop-master:9000/benchmarks/MRBench/mr_output/output_-261253429/_temporary/1/_temporary/attempt_1609033008462_0023_m_000000_0"}, {time: "2020-12-27T02:01:48.217Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED"}, {time: "2020-12-27T02:01:48.218Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.MapTask: Processing split: hdfs://hadoop-master:9000/benchmarks/MRBench/mr_input/input_990096104.txt:0+2"}, {time: "2020-12-27T02:01:48.222Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave1 to /default-rack"}, {time: "2020-12-27T02:01:48.222Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave2 to /default-rack"}, {time: "2020-12-27T02:01:48.222Z", level: "INFO", log: "[Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 1 failures on node hadoop-slave1"}, {time: "2020-12-27T02:01:48.223Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_1 TaskAttempt Transitioned from NEW to UNASSIGNED"}, {time: "2020-12-27T02:01:48.224Z", level: "INFO", log: "[Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1609033008462_0023_m_000000_1 to list of failed maps"}, {time: "2020-12-27T02:01:48.239Z", level: "WARN", log: "[main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.241Z", level: "INFO", log: "[IPC Server handler 2 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1609033008462_0023_m_000001_0 is : 0.0"}, {time: "2020-12-27T02:01:48.241Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task"}, {time: "2020-12-27T02:01:48.245Z", level: "WARN", log: "[main] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://hadoop-master:9000/benchmarks/MRBench/mr_output/output_-261253429/_temporary/1/_temporary/attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:48.247Z", level: "FATAL", log: "[IPC Server handler 3 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1609033008462_0023_m_000001_0 - exited : java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.247Z", level: "INFO", log: "[IPC Server handler 3 on 36781] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1609033008462_0023_m_000001_0: Error: java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.248Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1609033008462_0023_m_000001_0: Error: java.io.FileNotFoundException: File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1228)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1213)\n\tat org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1201)\n\tat org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:306)\n\tat org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:272)\n\tat org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:264)\n\tat org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1526)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:303)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:299)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:109)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:169)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /benchmarks/MRBench/mr_input/input_990096104.txt\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1828)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1799)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1712)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:587)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1475)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.getBlockLocations(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:1226)\n\t... 21 more\n"}, {time: "2020-12-27T02:01:48.248Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_0 TaskAttempt Transitioned from RUNNING to FAIL_CONTAINER_CLEANUP"}, {time: "2020-12-27T02:01:48.250Z", level: "INFO", log: "[ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_CLEANUP for container container_1609033008462_0023_01_000003 taskAttempt attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:48.250Z", level: "INFO", log: "[ContainerLauncher #3] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: KILLING attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:48.250Z", level: "INFO", log: "[ContainerLauncher #3] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : hadoop-slave1:39617"}, {time: "2020-12-27T02:01:48.256Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_0 TaskAttempt Transitioned from FAIL_CONTAINER_CLEANUP to FAIL_TASK_CLEANUP"}, {time: "2020-12-27T02:01:48.257Z", level: "INFO", log: "[CommitterEvent Processor #2] org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Processing the event EventType: TASK_ABORT"}, {time: "2020-12-27T02:01:48.258Z", level: "WARN", log: "[CommitterEvent Processor #2] org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter: Could not delete hdfs://hadoop-master:9000/benchmarks/MRBench/mr_output/output_-261253429/_temporary/1/_temporary/attempt_1609033008462_0023_m_000001_0"}, {time: "2020-12-27T02:01:48.258Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_0 TaskAttempt Transitioned from FAIL_TASK_CLEANUP to FAILED"}, {time: "2020-12-27T02:01:48.259Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave1 to /default-rack"}, {time: "2020-12-27T02:01:48.259Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave2 to /default-rack"}, {time: "2020-12-27T02:01:48.259Z", level: "INFO", log: "[Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: 2 failures on node hadoop-slave1"}, {time: "2020-12-27T02:01:48.259Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_1 TaskAttempt Transitioned from NEW to UNASSIGNED"}, {time: "2020-12-27T02:01:48.259Z", level: "INFO", log: "[Thread-50] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Added attempt_1609033008462_0023_m_000001_1 to list of failed maps"}, {time: "2020-12-27T02:01:48.970Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Before Scheduling: PendingReds:1 ScheduledMaps:2 ScheduledReds:0 AssignedMaps:2 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:2 RackLocal:0"}, {time: "2020-12-27T02:01:48.976Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor: getResources() for application_1609033008462_0023: ask=1 release= 0 newContainers=0 finishedContainers=1 resourcelimit=<memory:13312, vCores:1> knownNMs=2"}, {time: "2020-12-27T02:01:48.976Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1609033008462_0023_01_000002"}, {time: "2020-12-27T02:01:48.976Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:13312, vCores:1>"}, {time: "2020-12-27T02:01:48.976Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1609033008462_0023_m_000000_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n"}, {time: "2020-12-27T02:01:48.976Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1"}, {time: "2020-12-27T02:01:48.976Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:2 ScheduledReds:0 AssignedMaps:1 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:2 ContRel:0 HostLocal:2 RackLocal:0"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Received completed container container_1609033008462_0023_01_000003"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Got allocated containers 2"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1609033008462_0023_m_000001_0: Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigning container Container: [ContainerId: container_1609033008462_0023_01_000004, NodeId: hadoop-slave1:39617, NodeHttpAddress: hadoop-slave1:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 172.19.0.3:39617 }, ] to fast fail map"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned from earlierFailedMaps"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1609033008462_0023_01_000004 to attempt_1609033008462_0023_m_000000_1"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigning container Container: [ContainerId: container_1609033008462_0023_01_000005, NodeId: hadoop-slave2:37877, NodeHttpAddress: hadoop-slave2:8042, Resource: <memory:1024, vCores:1>, Priority: 5, Token: Token { kind: ContainerToken, service: 172.19.0.4:37877 }, ] to fast fail map"}, {time: "2020-12-27T02:01:49.982Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned from earlierFailedMaps"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Assigned container container_1609033008462_0023_01_000005 to attempt_1609033008462_0023_m_000001_1"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Recalculating schedule, headroom=<memory:12288, vCores:1>"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Reduce slow start threshold not met. completedMapsForReduceSlowstart 1"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave1 to /default-rack"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: After Scheduling: PendingReds:1 ScheduledMaps:0 ScheduledReds:0 AssignedMaps:2 AssignedReds:0 CompletedMaps:0 CompletedReds:0 ContAlloc:4 ContRel:0 HostLocal:2 RackLocal:0"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED"}, {time: "2020-12-27T02:01:49.983Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.yarn.util.RackResolver: Resolved hadoop-slave2 to /default-rack"}, {time: "2020-12-27T02:01:49.984Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_1 TaskAttempt Transitioned from UNASSIGNED to ASSIGNED"}, {time: "2020-12-27T02:01:49.984Z", level: "INFO", log: "[ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1609033008462_0023_01_000004 taskAttempt attempt_1609033008462_0023_m_000000_1"}, {time: "2020-12-27T02:01:49.984Z", level: "INFO", log: "[ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1609033008462_0023_m_000000_1"}, {time: "2020-12-27T02:01:49.984Z", level: "INFO", log: "[ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Processing the event EventType: CONTAINER_REMOTE_LAUNCH for container container_1609033008462_0023_01_000005 taskAttempt attempt_1609033008462_0023_m_000001_1"}, {time: "2020-12-27T02:01:49.984Z", level: "INFO", log: "[ContainerLauncher #4] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : hadoop-slave1:39617"}, {time: "2020-12-27T02:01:49.984Z", level: "INFO", log: "[ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Launching attempt_1609033008462_0023_m_000001_1"}, {time: "2020-12-27T02:01:49.985Z", level: "INFO", log: "[ContainerLauncher #5] org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy: Opening proxy : hadoop-slave2:37877"}, {time: "2020-12-27T02:01:49.992Z", level: "INFO", log: "[ContainerLauncher #4] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1609033008462_0023_m_000000_1 : 13562"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1609033008462_0023_m_000000_1] using containerId: [container_1609033008462_0023_01_000004 on NM: [hadoop-slave1:39617]"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[ContainerLauncher #5] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Shuffle port returned by ContainerManager for attempt_1609033008462_0023_m_000001_1 : 13562"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000000_1 TaskAttempt Transitioned from ASSIGNED to RUNNING"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: TaskAttempt: [attempt_1609033008462_0023_m_000001_1] using containerId: [container_1609033008462_0023_01_000005 on NM: [hadoop-slave2:37877]"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1609033008462_0023_m_000001_1 TaskAttempt Transitioned from ASSIGNED to RUNNING"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1609033008462_0023_m_000000"}, {time: "2020-12-27T02:01:49.993Z", level: "INFO", log: "[AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator: ATTEMPT_START task_1609033008462_0023_m_000001"}]
        },
        {
            data: [{time: "2020-12-27T02:30:01.723Z", level: "WARN", log: "[main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.net.NoRouteToHostException: No Route to Host from  hadoop-slave1/172.19.0.3 to hadoop-master:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy12.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:296)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy13.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1652)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1689)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1624)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:444)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:444)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:387)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:909)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:890)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:132)\n\tat org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.<init>(ReduceTask.java:540)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:614)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.net.NoRouteToHostException: No route to host\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\t... 31 more\n"}, {time: "2020-12-27T02:30:01.725Z", level: "INFO", log: "[IPC Server handler 7 on 37213] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1609036064649_0008_r_000000_0 is : 0.0"}, {time: "2020-12-27T02:30:01.727Z", level: "INFO", log: "[main] org.apache.hadoop.mapred.Task: Runnning cleanup for the task"}]
        },
        {
            data: [{time: "2020-12-27T02:31:14.720Z", level: "INFO", log: "SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1609036064649_0010_000001 (auth:SIMPLE)"}, {time: "2020-12-27T02:31:14.721Z", level: "INFO", log: "SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for appattempt_1609036064649_0010_000001 (auth:SIMPLE)"}, {time: "2020-12-27T02:31:14.723Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1609036064649_0010_01_000002 by user root"}, {time: "2020-12-27T02:31:14.724Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1609036064649_0010 transitioned from NEW to INITING"}, {time: "2020-12-27T02:31:14.724Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Creating a new application reference for app application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.724Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1609036064649_0010_01_000002 to application application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.724Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=root\tIP=172.19.0.4\tOPERATION=Start Container Request\tTARGET=ContainerManageImpl\tRESULT=SUCCESS\tAPPID=application_1609036064649_0010\tCONTAINERID=container_1609036064649_0010_01_000002"}, {time: "2020-12-27T02:31:14.724Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Application application_1609036064649_0010 transitioned from INITING to RUNNING"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1609036064649_0010"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Adding container_1609036064649_0010_01_000003 to application application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.jar transitioned from INIT to DOWNLOADING"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=root\tIP=172.19.0.4\tOPERATION=Start Container Request\tTARGET=ContainerManageImpl\tRESULT=SUCCESS\tAPPID=application_1609036064649_0010\tCONTAINERID=container_1609036064649_0010_01_000003"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1609036064649_0010_01_000002 transitioned from NEW to LOCALIZING"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.xml transitioned from INIT to DOWNLOADING"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Start request for container_1609036064649_0010_01_000003 by user root"}, {time: "2020-12-27T02:31:14.725Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1609036064649_0010_01_000002"}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event APPLICATION_INIT for appId application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1609036064649_0010_01_000002.tokens. Credentials list: "}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.mapred.ShuffleHandler: Added token for job_1609036064649_0010"}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got APPLICATION_INIT for service mapreduce_shuffle"}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Created localizer for container_1609036064649_0010_01_000003"}, {time: "2020-12-27T02:31:14.726Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1609036064649_0010_01_000003 transitioned from NEW to LOCALIZING"}, {time: "2020-12-27T02:31:14.727Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1609036064649_0010_01_000003.tokens. Credentials list: "}, {time: "2020-12-27T02:31:14.733Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user root"}, {time: "2020-12-27T02:31:14.734Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1609036064649_0010_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010/container_1609036064649_0010_01_000002.tokens"}, {time: "2020-12-27T02:31:14.735Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.740Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Initializing user root"}, {time: "2020-12-27T02:31:14.741Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010"}, {time: "2020-12-27T02:31:14.741Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1609036064649_0010_01_000003.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010/container_1609036064649_0010_01_000003.tokens"}, {time: "2020-12-27T02:31:17.815Z", level: "WARN", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: { hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.jar, 1609036267407, PATTERN, (?:classes/|lib/).* } failed: No Route to Host from  hadoop-slave1/172.19.0.3 to hadoop-master:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\njava.net.NoRouteToHostException: No Route to Host from  hadoop-slave1/172.19.0.3 to hadoop-master:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy76.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy77.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)\n\tat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:358)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\t... 30 more\nCaused by: No route to host\njava.net.NoRouteToHostException: No route to host\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy76.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy77.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)\n\tat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:358)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"}, {time: "2020-12-27T02:31:17.815Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1609036064649_0010_01_000003 transitioned from LOCALIZING to LOCALIZATION_FAILED"}, {time: "2020-12-27T02:31:17.815Z", level: "WARN", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: { hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.xml, 1609036267616, FILE, null } failed: No Route to Host from  hadoop-slave1/172.19.0.3 to hadoop-master:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\njava.net.NoRouteToHostException: No Route to Host from  hadoop-slave1/172.19.0.3 to hadoop-master:9000 failed on socket timeout exception: java.net.NoRouteToHostException: No route to host; For more details see:  http://wiki.apache.org/hadoop/NoRouteToHost\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:758)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1479)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy76.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy77.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)\n\tat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:358)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.NoRouteToHostException: No route to host\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\t... 30 more\nCaused by: No route to host\njava.net.NoRouteToHostException: No route to host\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:614)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:712)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:375)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1528)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1451)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy76.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.GeneratedMethodAccessor13.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy77.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253)\n\tat org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361)\n\tat org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:358)\n\tat org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"}, {time: "2020-12-27T02:31:17.815Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.jar(->/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010/filecache/10/job.jar) transitioned from DOWNLOADING to FAILED"}, {time: "2020-12-27T02:31:17.815Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalizedResource: Resource hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.xml(->/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1609036064649_0010/filecache/11/job.xml) transitioned from DOWNLOADING to FAILED"}, {time: "2020-12-27T02:31:17.815Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1609036064649_0010_01_000002 transitioned from LOCALIZING to LOCALIZATION_FAILED"}, {time: "2020-12-27T02:31:17.816Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Unknown localizer with localizerId container_1609036064649_0010_01_000002 is sending heartbeat. Ordering it to DIE"}, {time: "2020-12-27T02:31:17.816Z", level: "WARN", log: "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=root\tOPERATION=Container Finished - Failed\tTARGET=ContainerImpl\tRESULT=FAILURE\tDESCRIPTION=Container failed with state: LOCALIZATION_FAILED\tAPPID=application_1609036064649_0010\tCONTAINERID=container_1609036064649_0010_01_000002"}, {time: "2020-12-27T02:31:17.816Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl: Container container_1609036064649_0010_01_000002 sent RELEASE event on a resource request { hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.xml, 1609036267616, FILE, null } not present in cache."}, {time: "2020-12-27T02:31:17.816Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Unknown localizer with localizerId container_1609036064649_0010_01_000003 is sending heartbeat. Ordering it to DIE"}, {time: "2020-12-27T02:31:17.816Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl: Container container_1609036064649_0010_01_000003 sent RELEASE event on a resource request { hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.jar, 1609036267407, PATTERN, (?:classes/|lib/).* } not present in cache."}, {time: "2020-12-27T02:31:17.816Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl: Container container_1609036064649_0010_01_000002 sent RELEASE event on a resource request { hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.jar, 1609036267407, PATTERN, (?:classes/|lib/).* } not present in cache."}, {time: "2020-12-27T02:31:17.816Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.LocalResourcesTrackerImpl: Container container_1609036064649_0010_01_000003 sent RELEASE event on a resource request { hdfs://hadoop-master:9000/tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0010/job.xml, 1609036267616, FILE, null } not present in cache."}, {time: "2020-12-27T02:31:17.817Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1609036064649_0010_01_000003 transitioned from LOCALIZATION_FAILED to DONE"}, {time: "2020-12-27T02:31:17.817Z", level: "WARN", log: "org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger: USER=root\tOPERATION=Container Finished - Failed\tTARGET=ContainerImpl\tRESULT=FAILURE\tDESCRIPTION=Container failed with state: LOCALIZATION_FAILED\tAPPID=application_1609036064649_0010\tCONTAINERID=container_1609036064649_0010_01_000003"}, {time: "2020-12-27T02:31:17.817Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1609036064649_0010_01_000003 from application application_1609036064649_0010"}, {time: "2020-12-27T02:31:17.817Z", level: "WARN", log: "org.apache.hadoop.ipc.Client: interrupted waiting to send rpc request to server\njava.lang.InterruptedException\n\tat java.util.concurrent.FutureTask.awaitDone(FutureTask.java:400)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:187)\n\tat org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1059)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1454)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1412)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy73.heartbeat(Unknown Source)\n\tat org.apache.hadoop.yarn.server.nodemanager.api.impl.pb.client.LocalizationProtocolPBClientImpl.heartbeat(LocalizationProtocolPBClientImpl.java:62)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.localizeFiles(ContainerLocalizer.java:255)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer.runLocalization(ContainerLocalizer.java:169)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.startLocalizer(DefaultContainerExecutor.java:130)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerRunner.run(ResourceLocalizationService.java:1113)"}, {time: "2020-12-27T02:31:17.817Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl: Removing container_1609036064649_0010_01_000002 from application application_1609036064649_0010"}, {time: "2020-12-27T02:31:17.817Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_1609036064649_0010_01_000002 transitioned from LOCALIZATION_FAILED to DONE"}, {time: "2020-12-27T02:31:17.817Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1609036064649_0010"}, {time: "2020-12-27T02:31:17.818Z", level: "INFO", log: "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1609036064649_0010"}]
        },
        {
            data: [{time: "2020-12-27T02:41:15.128Z", level: "INFO", log: "org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742810_1986{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f14780ad-0342-426b-9f8f-a3f0e91f8a71:NORMAL:172.19.0.4:50010|RBW], ReplicaUC[[DISK]DS-83e35da6-281c-4a69-8efb-9e44ea566600:NORMAL:172.19.0.3:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0038/job_1609036064649_0038_1.jhist"}, {time: "2020-12-27T02:41:15.255Z", level: "WARN", log: "org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})"}, {time: "2020-12-27T02:41:15.255Z", level: "WARN", log: "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy"}, {time: "2020-12-27T02:41:15.256Z", level: "INFO", log: "org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073742811_1987{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-83e35da6-281c-4a69-8efb-9e44ea566600:NORMAL:172.19.0.3:50010|RBW]]} for /tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0038/job_1609036064649_0038_1.jhist"}, {time: "2020-12-27T02:41:15.256Z", level: "WARN", log: "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}"}, {time: "2020-12-27T02:41:15.276Z", level: "WARN", log: "org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 2 but only 0 storage types can be selected (replication=2, selected=[], unavailable=[DISK], removed=[DISK, DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})"}, {time: "2020-12-27T02:41:15.276Z", level: "WARN", log: "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy"}, {time: "2020-12-27T02:41:15.276Z", level: "WARN", log: "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 2 to reach 2 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=true) All required storage types are unavailable:  unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}"}, {time: "2020-12-27T02:41:15.278Z", level: "INFO", log: "org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from 172.19.0.4:59644 Call#32 Retry#0\njava.io.IOException: File /tmp/hadoop-yarn/staging/root/.staging/job_1609036064649_0038/job_1609036064649_0038_1.jhist could only be replicated to 0 nodes instead of minReplication (=1).  There are 2 datanode(s) running and 2 node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1547)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3107)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3031)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:724)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)"}, {time: "2020-12-27T02:41:15.922Z", level: "WARN", log: "org.apache.hadoop.hdfs.server.namen2020-12-27 02:41:17,564 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 172.19.0.4:59644 Call#41 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/root/output/_temporary/1/_temporary/attempt_1609036064649_0038_m_000001_1. Name node is in safe mode.\nResources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off."}, {time: "2020-12-27T02:41:17.574Z", level: "INFO", log: "org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.delete from 172.19.0.4:59644 Call#42 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot delete /user/root/output/_temporary/1/_temporary/attempt_1609036064649_0038_m_000000_1. Name node is in safe mode.\nResources are low on NN. Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off."}]
        }
    ],
}
export default new Vuex.Store({
    state
})